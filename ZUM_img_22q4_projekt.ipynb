{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 23 23:24:11 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.43.04    Driver Version: 515.43.04    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:01:00.0  On |                  N/A |\n",
      "| 45%   30C    P8    N/A /  75W |    469MiB /  4096MiB |     17%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1142      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    0   N/A  N/A     72352      G   /usr/lib/xorg/Xorg                165MiB |\n",
      "|    0   N/A  N/A     72483      G   /usr/bin/gnome-shell               41MiB |\n",
      "|    0   N/A  N/A     72958      G   ...AAAAAAAAA= --shared-files       57MiB |\n",
      "|    0   N/A  N/A     77761      G   /usr/lib/firefox/firefox          133MiB |\n",
      "|    0   N/A  N/A     87837      G   gnome-control-center                1MiB |\n",
      "|    0   N/A  N/A     93339      G   ...526599576985143180,131072       22MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpu_options = tf.compat.v1.GPUOptions(allow_growth=True)\n",
    "session = tf.compat.v1.InteractiveSession(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IImW6xGnwobW"
   },
   "source": [
    "# Obraz - transfer learning - praca domowa\n",
    "Ostatnia aktualizacja: 2022.12.10\n",
    "\n",
    "Z dokumentacji [Keras](https://keras.io/guides/transfer_learning/): *Transfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem.*\n",
    "\n",
    "Głównym celem pracy domowej jest stworzenie klasyfikatora, który będzie odróżniał zdjęcia obiektów w wybranym przez nas zbiorze. Użyjemy do tego jednego z [gotowych modeli Keras](https://keras.io/api/applications/), wytrenowanego wcześniej na zbiorze Imagenet. \n",
    "\n",
    "Przydatne źródła:\n",
    "- [transfer learning vgg16 + tf_flowers](https://towardsdatascience.com/transfer-learning-with-vgg16-and-keras-50ea161580b4)\n",
    "- [Keras - transfer learning](https://keras.io/guides/transfer_learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_VaMT6Ixv2P"
   },
   "source": [
    "### Wybrany zbiór danych:\n",
    "\n",
    "- [znaki drogowe](https://www.kaggle.com/datasets/valentynsichkar/traffic-signs-1-million-images-for-classification). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "filename = \"dataset_ts_original.hdf5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    \n",
    "    x_test = f['x_test'][()].astype('float32')\n",
    "    x_train = f['x_train'][()].astype('float32')\n",
    "    y_test = f['y_test'][()].astype('float32')\n",
    "    y_train = f['y_train'][()].astype('float32')\n",
    "    x_validation = f['x_validation'][()].astype('float32')\n",
    "    y_validation = f['y_validation'][()].astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LgWoGVFJ2BA5"
   },
   "source": [
    "### Zadanie 1: Wytrenuj model na swoich danych ###\n",
    "**(Zadanie na ocenę 3)**\n",
    "\n",
    "Wytrenuj wybrany model na swoich danych. Omów eksperyment i wyniki (100 słów). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3111, 48, 48, 3)\n",
      "(36288, 48, 48, 3)\n",
      "(12440, 48, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "assert x_test.shape[1:] == x_train.shape[1:] == x_validation.shape[1:]\n",
    "print(x_test.shape)\n",
    "print(x_train.shape)\n",
    "print(x_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_validation_resized = tf.image.resize(x_validation[:300], (150, 150)).numpy()\n",
    "# x_test_resized = tf.image.resize(x_test[:50], (150, 150)).numpy()\n",
    "# x_train_resized = tf.image.resize(x_train[:1500], (150, 150)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAo9-ecevKWT"
   },
   "source": [
    "### Wczytaj wytrenowany model do klasyfikacji obrazu. ###\n",
    "Może to być jeden z gotowych [modeli dostępnych w Keras](https://keras.io/api/applications/). Wczytujemy go z wytrenowanymi już wcześniej wagami na Imagenecie (weights='imagenet'). \n",
    "\n",
    "Model możemy wczytać bez ostatnich warstw (include_top=False) i dodać je potem ręcznie, dostosowane do liczby klas w naszym zbiorze. Imagenet ma 1000 klas, my prawdopodobnie będziemy mieć ich mniej. \n",
    "\n",
    "Pamiętaj, żeby wyłączyć lub ograniczyć trening części modelu z wytrenowanymi już wagami (trainable=False). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IccXldfvxvYH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=x_train[0].shape)\n",
    "base_model.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "dense_layer_1 = layers.Dense(150, activation='relu')\n",
    "prediction_layer = layers.Dense(43, activation='softmax')\n",
    "\n",
    "\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 1, 1, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 150)               76950     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 43)                6493      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,798,131\n",
      "Trainable params: 83,443\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36288,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_train = pd.get_dummies(y_train).to_numpy()\n",
    "y_validation = pd.get_dummies(y_validation).to_numpy()\n",
    "y_test = pd.get_dummies(y_test).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "908/908 [==============================] - 41s 40ms/step - loss: 2.0640 - accuracy: 0.6008 - val_loss: 1.0568 - val_accuracy: 0.7152\n",
      "Epoch 2/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.7532 - accuracy: 0.7740 - val_loss: 0.7749 - val_accuracy: 0.7852\n",
      "Epoch 3/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.5510 - accuracy: 0.8295 - val_loss: 0.7730 - val_accuracy: 0.7834\n",
      "Epoch 4/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.4541 - accuracy: 0.8551 - val_loss: 0.6933 - val_accuracy: 0.8143\n",
      "Epoch 5/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.3894 - accuracy: 0.8763 - val_loss: 0.6481 - val_accuracy: 0.8330\n",
      "Epoch 6/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.3391 - accuracy: 0.8892 - val_loss: 0.6649 - val_accuracy: 0.8325\n",
      "Epoch 7/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.3023 - accuracy: 0.9006 - val_loss: 0.6673 - val_accuracy: 0.8418\n",
      "Epoch 8/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.2740 - accuracy: 0.9114 - val_loss: 0.6449 - val_accuracy: 0.8455\n",
      "Epoch 9/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.2414 - accuracy: 0.9219 - val_loss: 0.6529 - val_accuracy: 0.8541\n",
      "Epoch 10/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.2299 - accuracy: 0.9261 - val_loss: 0.6665 - val_accuracy: 0.8563\n",
      "Epoch 11/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.2144 - accuracy: 0.9311 - val_loss: 0.7492 - val_accuracy: 0.8538\n",
      "Epoch 12/50\n",
      "908/908 [==============================] - 36s 40ms/step - loss: 0.1930 - accuracy: 0.9385 - val_loss: 0.7461 - val_accuracy: 0.8494\n",
      "Epoch 13/50\n",
      "908/908 [==============================] - 36s 40ms/step - loss: 0.1832 - accuracy: 0.9402 - val_loss: 0.7188 - val_accuracy: 0.8637\n",
      "Epoch 14/50\n",
      "908/908 [==============================] - 37s 40ms/step - loss: 0.1700 - accuracy: 0.9447 - val_loss: 0.7583 - val_accuracy: 0.8607\n",
      "Epoch 15/50\n",
      "908/908 [==============================] - 35s 39ms/step - loss: 0.1636 - accuracy: 0.9467 - val_loss: 0.8359 - val_accuracy: 0.8495\n",
      "Epoch 16/50\n",
      "908/908 [==============================] - 36s 40ms/step - loss: 0.1527 - accuracy: 0.9498 - val_loss: 0.8321 - val_accuracy: 0.8541\n",
      "Epoch 17/50\n",
      "908/908 [==============================] - 36s 39ms/step - loss: 0.1481 - accuracy: 0.9530 - val_loss: 0.8617 - val_accuracy: 0.8573\n",
      "Epoch 18/50\n",
      "908/908 [==============================] - 36s 40ms/step - loss: 0.1388 - accuracy: 0.9562 - val_loss: 0.9146 - val_accuracy: 0.8582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f67873fe9a0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5,  restore_best_weights=True)\n",
    "\n",
    "model.fit(x_train, y_train, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 3.4126278e-36, 2.4456801e-34, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 1.3031604e-32,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 7.4905679e-31, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 1.0549303e-37, 0.0000000e+00,\n",
       "        6.7797821e-36, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 0.0000000e+00, 0.0000000e+00,\n",
       "        4.5613204e-38, 0.0000000e+00, 7.4406331e-34, 0.0000000e+00,\n",
       "        0.0000000e+00, 7.9408963e-36, 0.0000000e+00],\n",
       "       [4.1512739e-12, 6.6080820e-06, 7.8983510e-07, 1.5811435e-06,\n",
       "        1.3831764e-05, 3.1484937e-08, 3.7811067e-11, 2.5975558e-10,\n",
       "        3.4432971e-06, 2.7480095e-08, 1.0203946e-10, 5.3578051e-04,\n",
       "        9.3616372e-06, 3.6711779e-06, 2.4849458e-08, 2.5991131e-09,\n",
       "        2.7941237e-06, 2.9742338e-07, 4.2466547e-02, 2.8280127e-09,\n",
       "        1.5974403e-05, 3.7102326e-04, 1.7806007e-05, 2.6954850e-02,\n",
       "        9.2621642e-01, 3.2660561e-03, 4.1740557e-05, 5.0302756e-10,\n",
       "        1.9206093e-06, 5.1929700e-08, 2.1142536e-05, 2.2974058e-08,\n",
       "        1.0886535e-07, 6.0515418e-08, 3.7845447e-09, 4.5002225e-06,\n",
       "        2.9000693e-08, 4.1395277e-05, 1.9380648e-06, 3.2234115e-11,\n",
       "        4.6470432e-12, 1.4597082e-07, 1.0902576e-11]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_train[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-jcAR_Y3mwN"
   },
   "source": [
    "*TODO: omówienie (100 słów)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ceCsNpA42H7Z"
   },
   "source": [
    "omówienie will be here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMwQtLDyyBIn"
   },
   "source": [
    "### Zadanie 2: Dodatkowe sieci ###\n",
    "**(Zadanie na ocenę 4, po wykonaniu  zadania 1)**\n",
    "\n",
    "Przeprowadź to samo na dwóch dodatkowych sieciach i omów wyniki (100 słów). \n",
    "\n",
    "Czyli jeśli w zadaniu 1 użyliśmy np. VGG to teraz wybieramy sobie np. ResNet i MobileNet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try pytorch lightning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/shreydan/resnet50-pytorch-lightning-kfolds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "filename = \"dataset_ts_original.hdf5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    x_test = f['x_test'][()]\n",
    "    x_train = f['x_train'][()]\n",
    "    y_test = f['y_test'][()]\n",
    "    y_train = f['y_train'][()]\n",
    "    x_validation = f['x_validation'][()]\n",
    "    y_validation = f['y_validation'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "# train\n",
    "tensor_x_train = torch.from_numpy(x_train).permute(0, 3, 1, 2)\n",
    "tensor_y_train = torch.Tensor(y_train)\n",
    "#tensor_y_train = tensor_y_train.type(torch.FloatTensor) # doesn't help, need to do casting in training step\n",
    "tensor_dataset_train = TensorDataset(tensor_x_train, tensor_y_train)\n",
    "\n",
    "# val\n",
    "tensor_x_val = torch.from_numpy(x_validation).permute(0, 3, 1, 2)\n",
    "tensor_y_val = torch.Tensor(y_validation)\n",
    "#tensor_y_val = tensor_y_val.type(torch.FloatTensor)\n",
    "tensor_dataset_val = TensorDataset(tensor_x_val, tensor_y_val)\n",
    "\n",
    "#test\n",
    "tensor_x_test = torch.from_numpy(x_test).permute(0, 3, 1, 2)\n",
    "tensor_y_test = torch.Tensor(y_test)\n",
    "#tensor_y_test = tensor_y_test.type(torch.FloatTensor) \n",
    "tensor_dataset_test = TensorDataset(tensor_x_test, tensor_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class ResNet50Model(pl.LightningModule):\n",
    "    def __init__(self, pretrained=True, in_channels = 3, num_classes = 16, lr=3e-4, freeze=False):\n",
    "        super(ResNet50Model, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.model = models.resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        \n",
    "        if freeze:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.model.fc = nn.Sequential(\n",
    "            nn.Linear(self.model.fc.in_features, 128),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, self.num_classes)\n",
    "        )\n",
    "        \n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=self.num_classes)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2)\n",
    "        return [optimizer], [scheduler]\n",
    "    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x, y = batch\n",
    "        \n",
    "        preds = self.model(x)\n",
    "\n",
    "        y = y.type(torch.LongTensor)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        loss = self.loss_fn(preds, y)\n",
    "        self.train_acc(torch.argmax(preds, dim=1), y)\n",
    "        \n",
    "        self.log('train_loss', loss.item(), on_epoch=True)\n",
    "        self.log('train_acc', self.train_acc, on_epoch=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        x,y = batch\n",
    "        \n",
    "        preds = self.model(x)\n",
    "        \n",
    "        y = y.type(torch.LongTensor)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        loss = self.loss_fn(preds, y)\n",
    "        self.val_acc(torch.argmax(preds, dim=1), y)\n",
    "        \n",
    "        self.log('val_loss', loss.item(), on_epoch=True)\n",
    "        self.log('val_acc', self.val_acc, on_epoch=True)\n",
    "        \n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \n",
    "        x,y = batch\n",
    "        preds = self.model(x)\n",
    "        self.test_acc(torch.argmax(preds, dim=1), y)\n",
    "        \n",
    "        self.log('test_acc', self.test_acc, on_epoch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freeze=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "======================================== FOLD 1 / 4 ========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | ResNet             | 23.8 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | train_acc | MulticlassAccuracy | 0     \n",
      "3 | val_acc   | MulticlassAccuracy | 0     \n",
      "4 | test_acc  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "23.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.8 M    Total params\n",
      "95.103    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b752031963647e3a5f574e78a5da753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613452766d9743849d2663e694e41767",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9980713725090027\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Train Loss: 0.0002568479103501886 | Train Accuracy: 1.0\n",
      "Val Loss: 0.004727860447019339 | Val Accuracy: 0.9988746047019958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | ResNet             | 23.8 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | train_acc | MulticlassAccuracy | 0     \n",
      "3 | val_acc   | MulticlassAccuracy | 0     \n",
      "4 | test_acc  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "23.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.8 M    Total params\n",
      "95.103    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "======================================== FOLD 2 / 4 ========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e765b4fca148d99756ffc43b4e914c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c39e8600c4e4b42901bb8fa01365a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9980713725090027\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Train Loss: 0.0002971006906591356 | Train Accuracy: 1.0\n",
      "Val Loss: 0.007029504049569368 | Val Accuracy: 0.9982315301895142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | ResNet             | 23.8 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | train_acc | MulticlassAccuracy | 0     \n",
      "3 | val_acc   | MulticlassAccuracy | 0     \n",
      "4 | test_acc  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "23.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.8 M    Total params\n",
      "95.103    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "======================================== FOLD 3 / 4 ========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9204b0c4ada1469bb736b7457cb0c78b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d4c69055a24cafa9056d17b1264e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc             0.999035656452179\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Train Loss: 0.00028609790024347603 | Train Accuracy: 0.9999724626541138\n",
      "Val Loss: 0.005856333766132593 | Val Accuracy: 0.9983118772506714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | ResNet             | 23.8 M\n",
      "1 | loss_fn   | CrossEntropyLoss   | 0     \n",
      "2 | train_acc | MulticlassAccuracy | 0     \n",
      "3 | val_acc   | MulticlassAccuracy | 0     \n",
      "4 | test_acc  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "23.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "23.8 M    Total params\n",
      "95.103    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "======================================== FOLD 4 / 4 ========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3547e5c6cb34a85946ae9c16ce0dab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c6277dd68ba40a895d7937296e61306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc            0.9993571043014526\n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "Train Loss: 0.0001943597017088905 | Train Accuracy: 1.0\n",
      "Val Loss: 0.004916341044008732 | Val Accuracy: 0.9986334443092346\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "num_folds = 4\n",
    "kf = StratifiedKFold(num_folds)\n",
    "lr = 3e-4\n",
    "logs = dict()\n",
    "\n",
    "for fold, (train_fold, val_fold) in enumerate(kf.split(X=x_train, y=y_train), start=1):\n",
    "    train_dataloader = torch.utils.data.DataLoader(tensor_dataset_train,batch_size=64,num_workers=8)\n",
    "    val_dataloader = torch.utils.data.DataLoader(tensor_dataset_val, batch_size=64,num_workers=8)\n",
    "    test_dataloader = torch.utils.data.DataLoader(tensor_dataset_test, batch_size=128, num_workers=8)\n",
    "    \n",
    "    model = ResNet50Model(num_classes=43, freeze=False)\n",
    "    trainer = pl.Trainer(accelerator='gpu', \n",
    "                         max_epochs=10,\n",
    "                         callbacks=[\n",
    "                         EarlyStopping(monitor=\"val_loss\", \n",
    "                                       mode=\"min\",\n",
    "                                       patience=2)])\n",
    "    \n",
    "    model.hparams.lr = lr\n",
    "    \n",
    "    print(f\"\\n\\n\\n{'=='*20} FOLD {fold} / {num_folds} {'=='*20}\")\n",
    "    \n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    metrics = trainer.logged_metrics\n",
    "    trainer.test(model, test_dataloader)\n",
    "    \n",
    "    logs[f'fold{fold}'] = {\n",
    "        'train_loss': metrics['train_loss_epoch'].item(),\n",
    "        'val_loss': metrics['val_loss'].item(),\n",
    "        'train_acc': metrics['train_acc_epoch'].item(),\n",
    "        'val_acc': metrics['val_acc'].item()\n",
    "    }\n",
    "    \n",
    "    print(f\"Train Loss: {logs[f'fold{fold}']['train_loss']} | Train Accuracy: {logs[f'fold{fold}']['train_acc']}\")\n",
    "    print(f\"Val Loss: {logs[f'fold{fold}']['val_loss']} | Val Accuracy: {logs[f'fold{fold}']['val_acc']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "freeze=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "num_folds = 4\n",
    "kf = StratifiedKFold(num_folds)\n",
    "lr = 3e-4\n",
    "logs = dict()\n",
    "\n",
    "for fold, (train_fold, val_fold) in enumerate(kf.split(X=x_train, y=y_train), start=1):\n",
    "    train_dataloader = torch.utils.data.DataLoader(tensor_dataset_train,batch_size=64,num_workers=8)\n",
    "    val_dataloader = torch.utils.data.DataLoader(tensor_dataset_val, batch_size=64,num_workers=8)\n",
    "    test_dataloader = torch.utils.data.DataLoader(tensor_dataset_test, batch_size=128, num_workers=8)\n",
    "    \n",
    "    model = ResNet50Model(num_classes=43, freeze=True)\n",
    "    trainer = pl.Trainer(accelerator='gpu', \n",
    "                         max_epochs=10,\n",
    "                         callbacks=[\n",
    "                         EarlyStopping(monitor=\"val_loss\", \n",
    "                                       mode=\"min\",\n",
    "                                       patience=2)])\n",
    "    \n",
    "    model.hparams.lr = lr\n",
    "    \n",
    "    print(f\"\\n\\n\\n{'=='*20} FOLD {fold} / {num_folds} {'=='*20}\")\n",
    "    \n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    metrics = trainer.logged_metrics\n",
    "    trainer.test(model, test_dataloader)\n",
    "    \n",
    "    logs[f'fold{fold}'] = {\n",
    "        'train_loss': metrics['train_loss_epoch'].item(),\n",
    "        'val_loss': metrics['val_loss'].item(),\n",
    "        'train_acc': metrics['train_acc_epoch'].item(),\n",
    "        'val_acc': metrics['val_acc'].item()\n",
    "    }\n",
    "    \n",
    "    print(f\"Train Loss: {logs[f'fold{fold}']['train_loss']} | Train Accuracy: {logs[f'fold{fold}']['train_acc']}\")\n",
    "    print(f\"Val Loss: {logs[f'fold{fold}']['val_loss']} | Val Accuracy: {logs[f'fold{fold}']['val_acc']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 3, 7, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_parameter = next(model.parameters())\n",
    "input_shape = first_parameter.size()\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68eGKyM132xk"
   },
   "source": [
    "*TODO: omówienie (100 słów)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "omówienie will be here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3: Trening od zera i porównanie ###\n",
    "**(Zadanie na ocenę 5, po wykonaniu zadania 1 i 2)**\n",
    "\n",
    "Spróbuj skonstruować swój własny model i wytrenować go 'od zera' na tych samych danych. Porównaj i omów swój ekeperyment i wyniki (100 słów).\n",
    "\n",
    "reference: https://keras.io/api/applications/#usage-examples-for-image-classification-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "filename = \"dataset_ts_original.hdf5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f:\n",
    "    x_test = f['x_test'][()]\n",
    "    x_train = f['x_train'][()]\n",
    "    y_test = f['y_test'][()]\n",
    "    y_train = f['y_train'][()]\n",
    "    x_validation = f['x_validation'][()]\n",
    "    y_validation = f['y_validation'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_train = pd.get_dummies(y_train).to_numpy()\n",
    "y_validation = pd.get_dummies(y_validation).to_numpy()\n",
    "y_test = pd.get_dummies(y_test).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "\n",
    "cnn_model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=x_train[0].shape),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Conv2D(filters=16, kernel_size=3),\n",
    "    MaxPooling2D(pool_size=2),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(150, activation='relu'),\n",
    "    Dense(100,activation='relu'),\n",
    "    Dense(43,activation = 'softmax'),    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "\n",
    "cnn_model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=0.001), metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=15),\n",
    "             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "233/233 [==============================] - 11s 28ms/step - loss: 3.3748 - accuracy: 0.5381 - val_loss: 0.6167 - val_accuracy: 0.8361\n",
      "Epoch 2/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.5054 - accuracy: 0.8694 - val_loss: 0.3109 - val_accuracy: 0.9238\n",
      "Epoch 3/75\n",
      "233/233 [==============================] - 5s 21ms/step - loss: 0.2822 - accuracy: 0.9284 - val_loss: 0.2171 - val_accuracy: 0.9473\n",
      "Epoch 4/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.1915 - accuracy: 0.9517 - val_loss: 0.1673 - val_accuracy: 0.9634\n",
      "Epoch 5/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.1473 - accuracy: 0.9625 - val_loss: 0.1429 - val_accuracy: 0.9675\n",
      "Epoch 6/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.1211 - accuracy: 0.9679 - val_loss: 0.1321 - val_accuracy: 0.9707\n",
      "Epoch 7/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.1039 - accuracy: 0.9732 - val_loss: 0.1333 - val_accuracy: 0.9719\n",
      "Epoch 8/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0889 - accuracy: 0.9767 - val_loss: 0.1128 - val_accuracy: 0.9758\n",
      "Epoch 9/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0886 - accuracy: 0.9771 - val_loss: 0.1305 - val_accuracy: 0.9771\n",
      "Epoch 10/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0694 - accuracy: 0.9816 - val_loss: 0.1326 - val_accuracy: 0.9755\n",
      "Epoch 11/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0696 - accuracy: 0.9824 - val_loss: 0.1391 - val_accuracy: 0.9724\n",
      "Epoch 12/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0635 - accuracy: 0.9826 - val_loss: 0.1159 - val_accuracy: 0.9756\n",
      "Epoch 13/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0650 - accuracy: 0.9833 - val_loss: 0.1393 - val_accuracy: 0.9758\n",
      "Epoch 14/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0851 - accuracy: 0.9794 - val_loss: 0.1509 - val_accuracy: 0.9773\n",
      "Epoch 15/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0690 - accuracy: 0.9830 - val_loss: 0.1384 - val_accuracy: 0.9740\n",
      "Epoch 16/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0562 - accuracy: 0.9851 - val_loss: 0.0969 - val_accuracy: 0.9843\n",
      "Epoch 17/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0460 - accuracy: 0.9890 - val_loss: 0.1161 - val_accuracy: 0.9827\n",
      "Epoch 18/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0518 - accuracy: 0.9883 - val_loss: 0.1722 - val_accuracy: 0.9772\n",
      "Epoch 19/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0742 - accuracy: 0.9827 - val_loss: 0.1693 - val_accuracy: 0.9760\n",
      "Epoch 20/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0585 - accuracy: 0.9877 - val_loss: 0.1501 - val_accuracy: 0.9820\n",
      "Epoch 21/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0400 - accuracy: 0.9904 - val_loss: 0.1127 - val_accuracy: 0.9847\n",
      "Epoch 22/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0422 - accuracy: 0.9901 - val_loss: 0.1450 - val_accuracy: 0.9802\n",
      "Epoch 23/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0318 - accuracy: 0.9918 - val_loss: 0.1029 - val_accuracy: 0.9858\n",
      "Epoch 24/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0741 - accuracy: 0.9849 - val_loss: 0.1393 - val_accuracy: 0.9790\n",
      "Epoch 25/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0718 - accuracy: 0.9854 - val_loss: 0.1600 - val_accuracy: 0.9741\n",
      "Epoch 26/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0567 - accuracy: 0.9880 - val_loss: 0.1124 - val_accuracy: 0.9831\n",
      "Epoch 27/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0314 - accuracy: 0.9927 - val_loss: 0.0935 - val_accuracy: 0.9868\n",
      "Epoch 28/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0405 - accuracy: 0.9902 - val_loss: 0.1365 - val_accuracy: 0.9836\n",
      "Epoch 29/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0435 - accuracy: 0.9907 - val_loss: 0.1255 - val_accuracy: 0.9833\n",
      "Epoch 30/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0408 - accuracy: 0.9911 - val_loss: 0.1699 - val_accuracy: 0.9767\n",
      "Epoch 31/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0545 - accuracy: 0.9889 - val_loss: 0.1329 - val_accuracy: 0.9850\n",
      "Epoch 32/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0286 - accuracy: 0.9923 - val_loss: 0.1266 - val_accuracy: 0.9857\n",
      "Epoch 33/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0396 - accuracy: 0.9916 - val_loss: 0.1331 - val_accuracy: 0.9822\n",
      "Epoch 34/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0480 - accuracy: 0.9900 - val_loss: 0.1866 - val_accuracy: 0.9757\n",
      "Epoch 35/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0446 - accuracy: 0.9906 - val_loss: 0.1256 - val_accuracy: 0.9859\n",
      "Epoch 36/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0437 - accuracy: 0.9917 - val_loss: 0.1582 - val_accuracy: 0.9833\n",
      "Epoch 37/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0585 - accuracy: 0.9891 - val_loss: 0.1638 - val_accuracy: 0.9835\n",
      "Epoch 38/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0593 - accuracy: 0.9897 - val_loss: 0.2025 - val_accuracy: 0.9756\n",
      "Epoch 39/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0382 - accuracy: 0.9921 - val_loss: 0.1235 - val_accuracy: 0.9876\n",
      "Epoch 40/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0322 - accuracy: 0.9937 - val_loss: 0.1616 - val_accuracy: 0.9830\n",
      "Epoch 41/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0253 - accuracy: 0.9948 - val_loss: 0.1398 - val_accuracy: 0.9871\n",
      "Epoch 42/75\n",
      "233/233 [==============================] - 5s 20ms/step - loss: 0.0326 - accuracy: 0.9937 - val_loss: 0.1614 - val_accuracy: 0.9811\n"
     ]
    }
   ],
   "source": [
    "history = cnn_model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=156,\n",
    "    epochs=75,\n",
    "    verbose=1,\n",
    "    validation_data=(x_validation,y_validation),\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Train - Accuracy')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend()\n",
    "plt.title('Train - Loss Function')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Train - Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss : 0.1542\n",
      "Test Accuracy : 0.9826\n"
     ]
    }
   ],
   "source": [
    "score = cnn_model.evaluate(x_test,y_test,verbose=0)\n",
    "print('Test Loss : {:.4f}'.format(score[0]))\n",
    "print('Test Accuracy : {:.4f}'.format(score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "omówienie will be here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/examples/vision/vit_small_ds/#build-the-vit   \n",
    "https://arxiv.org/abs/2112.13492v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 43\n",
    "INPUT_SHAPE = (48, 48, 3)\n",
    "\n",
    "import h5py\n",
    "filename = \"dataset_ts_original.hdf5\"\n",
    "\n",
    "with h5py.File(filename, \"r\") as f: \n",
    "    x_test = f['x_test'][()].astype('float32')\n",
    "    x_train = f['x_train'][()].astype('float32')\n",
    "    y_test = f['y_test'][()].astype('float32')\n",
    "    y_train = f['y_train'][()].astype('float32')\n",
    "    x_validation = f['x_validation'][()].astype('float32')\n",
    "    y_validation = f['y_validation'][()].astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA\n",
    "BUFFER_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "# AUGMENTATION\n",
    "IMAGE_SIZE = 48\n",
    "PATCH_SIZE = 6\n",
    "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
    "\n",
    "# OPTIMIZER\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "# TRAINING\n",
    "EPOCHS = 50\n",
    "\n",
    "# ARCHITECTURE\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "TRANSFORMER_LAYERS = 8\n",
    "PROJECTION_DIM = 64\n",
    "NUM_HEADS = 4\n",
    "TRANSFORMER_UNITS = [\n",
    "    PROJECTION_DIM * 2,\n",
    "    PROJECTION_DIM,\n",
    "]\n",
    "MLP_HEAD_UNITS = [2048, 1024]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Normalization(),\n",
    "        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShiftedPatchTokenization(layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=IMAGE_SIZE,\n",
    "        patch_size=PATCH_SIZE,\n",
    "        num_patches=NUM_PATCHES,\n",
    "        projection_dim=PROJECTION_DIM,\n",
    "        vanilla=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.half_patch = patch_size // 2\n",
    "        self.flatten_patches = layers.Reshape((num_patches, -1))\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n",
    "\n",
    "    def crop_shift_pad(self, images, mode):\n",
    "        # Build the diagonally shifted images\n",
    "        if mode == \"left-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = 0\n",
    "            shift_width = 0\n",
    "        elif mode == \"left-down\":\n",
    "            crop_height = 0\n",
    "            crop_width = self.half_patch\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = 0\n",
    "        elif mode == \"right-up\":\n",
    "            crop_height = self.half_patch\n",
    "            crop_width = 0\n",
    "            shift_height = 0\n",
    "            shift_width = self.half_patch\n",
    "        else:\n",
    "            crop_height = 0\n",
    "            crop_width = 0\n",
    "            shift_height = self.half_patch\n",
    "            shift_width = self.half_patch\n",
    "\n",
    "        # Crop the shifted images and pad them\n",
    "        crop = tf.image.crop_to_bounding_box(\n",
    "            images,\n",
    "            offset_height=crop_height,\n",
    "            offset_width=crop_width,\n",
    "            target_height=self.image_size - self.half_patch,\n",
    "            target_width=self.image_size - self.half_patch,\n",
    "        )\n",
    "        shift_pad = tf.image.pad_to_bounding_box(\n",
    "            crop,\n",
    "            offset_height=shift_height,\n",
    "            offset_width=shift_width,\n",
    "            target_height=self.image_size,\n",
    "            target_width=self.image_size,\n",
    "        )\n",
    "        return shift_pad\n",
    "\n",
    "    def call(self, images):\n",
    "        if not self.vanilla:\n",
    "            # Concat the shifted images with the original image\n",
    "            images = tf.concat(\n",
    "                [\n",
    "                    images,\n",
    "                    self.crop_shift_pad(images, mode=\"left-up\"),\n",
    "                    self.crop_shift_pad(images, mode=\"left-down\"),\n",
    "                    self.crop_shift_pad(images, mode=\"right-up\"),\n",
    "                    self.crop_shift_pad(images, mode=\"right-down\"),\n",
    "                ],\n",
    "                axis=-1,\n",
    "            )\n",
    "        # Patchify the images and flatten it\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        flat_patches = self.flatten_patches(patches)\n",
    "        if not self.vanilla:\n",
    "            # Layer normalize the flat patches and linearly project it\n",
    "            tokens = self.layer_norm(flat_patches)\n",
    "            tokens = self.projection(tokens)\n",
    "        else:\n",
    "            # Linearly project the flat patches\n",
    "            tokens = self.projection(flat_patches)\n",
    "        return (tokens, patches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEncoder(layers.Layer):\n",
    "    def __init__(\n",
    "        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.position_embedding = layers.Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "\n",
    "    def call(self, encoded_patches):\n",
    "        encoded_positions = self.position_embedding(self.positions)\n",
    "        encoded_patches = encoded_patches + encoded_positions\n",
    "        return encoded_patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # The trainable temperature term. The initial value is\n",
    "        # the square root of the key dimension.\n",
    "        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n",
    "\n",
    "    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n",
    "        query = tf.multiply(query, 1.0 / self.tau)\n",
    "        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n",
    "        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n",
    "        attention_scores_dropout = self._dropout_layer(\n",
    "            attention_scores, training=training\n",
    "        )\n",
    "        attention_output = tf.einsum(\n",
    "            self._combine_equation, attention_scores_dropout, value\n",
    "        )\n",
    "        return attention_output, attention_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Build the diagonal attention mask\n",
    "diag_attn_mask = 1 - tf.eye(NUM_PATCHES)\n",
    "diag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier(vanilla=False):\n",
    "    inputs = layers.Input(shape=INPUT_SHAPE)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder()(tokens)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(TRANSFORMER_LAYERS):\n",
    "        # Layer normalization 1.\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        if not vanilla:\n",
    "            attention_output = MultiHeadAttentionLSA(\n",
    "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "            )(x1, x1, attention_mask=diag_attn_mask)\n",
    "        else:\n",
    "            attention_output = layers.MultiHeadAttention(\n",
    "                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n",
    "            )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = layers.Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = layers.Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = layers.Flatten()(representation)\n",
    "    representation = layers.Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = layers.Dense(NUM_CLASSES)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some code is taken from:\n",
    "# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\n",
    "class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n",
    "    ):\n",
    "        super(WarmUpCosine, self).__init__()\n",
    "\n",
    "        self.learning_rate_base = learning_rate_base\n",
    "        self.total_steps = total_steps\n",
    "        self.warmup_learning_rate = warmup_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.pi = tf.constant(np.pi)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        if self.total_steps < self.warmup_steps:\n",
    "            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n",
    "\n",
    "        cos_annealed_lr = tf.cos(\n",
    "            self.pi\n",
    "            * (tf.cast(step, tf.float32) - self.warmup_steps)\n",
    "            / float(self.total_steps - self.warmup_steps)\n",
    "        )\n",
    "        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n",
    "\n",
    "        if self.warmup_steps > 0:\n",
    "            if self.learning_rate_base < self.warmup_learning_rate:\n",
    "                raise ValueError(\n",
    "                    \"Learning_rate_base must be larger or equal to \"\n",
    "                    \"warmup_learning_rate.\"\n",
    "                )\n",
    "            slope = (\n",
    "                self.learning_rate_base - self.warmup_learning_rate\n",
    "            ) / self.warmup_steps\n",
    "            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n",
    "            learning_rate = tf.where(\n",
    "                step < self.warmup_steps, warmup_rate, learning_rate\n",
    "            )\n",
    "        return tf.where(\n",
    "            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n",
    "        )\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "    total_steps = int((len(x_train) / BATCH_SIZE) * EPOCHS)\n",
    "    warmup_epoch_percentage = 0.10\n",
    "    warmup_steps = int(total_steps * warmup_epoch_percentage)\n",
    "    scheduled_lrs = WarmUpCosine(\n",
    "        learning_rate_base=LEARNING_RATE,\n",
    "        total_steps=total_steps,\n",
    "        warmup_learning_rate=0.0,\n",
    "        warmup_steps=warmup_steps,\n",
    "    )\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_split=0.1,\n",
    "    )\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2042/2042 [==============================] - 239s 112ms/step - loss: 1.9745 - accuracy: 0.4562 - top-5-accuracy: 0.7725 - val_loss: 0.6602 - val_accuracy: 0.7812 - val_top-5-accuracy: 0.9760\n",
      "Epoch 2/50\n",
      "2042/2042 [==============================] - 228s 112ms/step - loss: 0.7682 - accuracy: 0.7563 - top-5-accuracy: 0.9636 - val_loss: 0.3270 - val_accuracy: 0.8851 - val_top-5-accuracy: 0.9909\n",
      "Epoch 3/50\n",
      "2042/2042 [==============================] - 228s 112ms/step - loss: 0.5121 - accuracy: 0.8373 - top-5-accuracy: 0.9829 - val_loss: 0.1869 - val_accuracy: 0.9325 - val_top-5-accuracy: 0.9961\n",
      "Epoch 4/50\n",
      "2042/2042 [==============================] - 228s 112ms/step - loss: 0.4011 - accuracy: 0.8747 - top-5-accuracy: 0.9883 - val_loss: 0.1303 - val_accuracy: 0.9573 - val_top-5-accuracy: 0.9967\n",
      "Epoch 5/50\n",
      "2042/2042 [==============================] - 228s 111ms/step - loss: 0.3457 - accuracy: 0.8935 - top-5-accuracy: 0.9914 - val_loss: 0.1158 - val_accuracy: 0.9631 - val_top-5-accuracy: 0.9975\n",
      "Epoch 6/50\n",
      "2042/2042 [==============================] - 227s 111ms/step - loss: 0.3060 - accuracy: 0.9060 - top-5-accuracy: 0.9926 - val_loss: 0.1464 - val_accuracy: 0.9518 - val_top-5-accuracy: 0.9986\n",
      "Epoch 7/50\n",
      "2042/2042 [==============================] - 228s 111ms/step - loss: 0.2846 - accuracy: 0.9107 - top-5-accuracy: 0.9927 - val_loss: 0.1030 - val_accuracy: 0.9661 - val_top-5-accuracy: 0.9981\n",
      "Epoch 8/50\n",
      "2042/2042 [==============================] - 227s 111ms/step - loss: 0.2774 - accuracy: 0.9157 - top-5-accuracy: 0.9930 - val_loss: 0.1223 - val_accuracy: 0.9592 - val_top-5-accuracy: 0.9981\n",
      "Epoch 9/50\n",
      "2042/2042 [==============================] - 228s 112ms/step - loss: 0.2516 - accuracy: 0.9211 - top-5-accuracy: 0.9945 - val_loss: 0.0847 - val_accuracy: 0.9724 - val_top-5-accuracy: 0.9978\n",
      "Epoch 10/50\n",
      "2042/2042 [==============================] - 234s 115ms/step - loss: 0.2642 - accuracy: 0.9197 - top-5-accuracy: 0.9937 - val_loss: 0.0901 - val_accuracy: 0.9694 - val_top-5-accuracy: 0.9981\n",
      "Epoch 11/50\n",
      "2042/2042 [==============================] - 232s 114ms/step - loss: 0.2692 - accuracy: 0.9208 - top-5-accuracy: 0.9930 - val_loss: 0.0962 - val_accuracy: 0.9691 - val_top-5-accuracy: 0.9981\n",
      "Epoch 12/50\n",
      "2042/2042 [==============================] - 228s 112ms/step - loss: 0.2320 - accuracy: 0.9287 - top-5-accuracy: 0.9944 - val_loss: 0.0898 - val_accuracy: 0.9691 - val_top-5-accuracy: 0.9975\n",
      "Epoch 13/50\n",
      "2042/2042 [==============================] - 227s 111ms/step - loss: 0.2376 - accuracy: 0.9287 - top-5-accuracy: 0.9953 - val_loss: 0.0875 - val_accuracy: 0.9691 - val_top-5-accuracy: 0.9992\n",
      "Epoch 14/50\n",
      "2042/2042 [==============================] - 228s 111ms/step - loss: 0.2445 - accuracy: 0.9261 - top-5-accuracy: 0.9946 - val_loss: 0.0927 - val_accuracy: 0.9716 - val_top-5-accuracy: 0.9978\n",
      "Epoch 15/50\n",
      "2042/2042 [==============================] - 227s 111ms/step - loss: 0.2347 - accuracy: 0.9290 - top-5-accuracy: 0.9947 - val_loss: 0.0855 - val_accuracy: 0.9716 - val_top-5-accuracy: 0.9983\n",
      "Epoch 16/50\n",
      "2042/2042 [==============================] - 228s 112ms/step - loss: 0.2275 - accuracy: 0.9297 - top-5-accuracy: 0.9952 - val_loss: 0.0645 - val_accuracy: 0.9777 - val_top-5-accuracy: 0.9992\n",
      "Epoch 17/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2339 - accuracy: 0.9290 - top-5-accuracy: 0.9949 - val_loss: 0.1044 - val_accuracy: 0.9645 - val_top-5-accuracy: 0.9989\n",
      "Epoch 18/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2367 - accuracy: 0.9304 - top-5-accuracy: 0.9941 - val_loss: 0.0928 - val_accuracy: 0.9669 - val_top-5-accuracy: 0.9986\n",
      "Epoch 19/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2255 - accuracy: 0.9327 - top-5-accuracy: 0.9951 - val_loss: 0.0677 - val_accuracy: 0.9769 - val_top-5-accuracy: 0.9989\n",
      "Epoch 20/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2228 - accuracy: 0.9310 - top-5-accuracy: 0.9961 - val_loss: 0.0948 - val_accuracy: 0.9686 - val_top-5-accuracy: 0.9986\n",
      "Epoch 21/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2366 - accuracy: 0.9287 - top-5-accuracy: 0.9942 - val_loss: 0.0937 - val_accuracy: 0.9719 - val_top-5-accuracy: 0.9981\n",
      "Epoch 22/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2072 - accuracy: 0.9371 - top-5-accuracy: 0.9962 - val_loss: 0.0874 - val_accuracy: 0.9749 - val_top-5-accuracy: 0.9975\n",
      "Epoch 23/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2146 - accuracy: 0.9344 - top-5-accuracy: 0.9956 - val_loss: 0.0807 - val_accuracy: 0.9733 - val_top-5-accuracy: 0.9986\n",
      "Epoch 24/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2188 - accuracy: 0.9345 - top-5-accuracy: 0.9957 - val_loss: 0.0872 - val_accuracy: 0.9744 - val_top-5-accuracy: 0.9992\n",
      "Epoch 25/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2128 - accuracy: 0.9356 - top-5-accuracy: 0.9951 - val_loss: 0.0846 - val_accuracy: 0.9741 - val_top-5-accuracy: 0.9997\n",
      "Epoch 26/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2196 - accuracy: 0.9334 - top-5-accuracy: 0.9952 - val_loss: 0.0985 - val_accuracy: 0.9680 - val_top-5-accuracy: 0.9989\n",
      "Epoch 27/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2145 - accuracy: 0.9349 - top-5-accuracy: 0.9959 - val_loss: 0.0848 - val_accuracy: 0.9722 - val_top-5-accuracy: 0.9983\n",
      "Epoch 28/50\n",
      "2042/2042 [==============================] - 226s 111ms/step - loss: 0.2135 - accuracy: 0.9378 - top-5-accuracy: 0.9946 - val_loss: 0.0744 - val_accuracy: 0.9774 - val_top-5-accuracy: 0.9986\n",
      "Epoch 29/50\n",
      "2042/2042 [==============================] - 228s 112ms/step - loss: 0.2158 - accuracy: 0.9348 - top-5-accuracy: 0.9958 - val_loss: 0.0789 - val_accuracy: 0.9788 - val_top-5-accuracy: 0.9989\n",
      "Epoch 30/50\n",
      "2042/2042 [==============================] - 228s 112ms/step - loss: 0.2188 - accuracy: 0.9336 - top-5-accuracy: 0.9947 - val_loss: 0.0934 - val_accuracy: 0.9689 - val_top-5-accuracy: 0.9983\n",
      "Epoch 31/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2095 - accuracy: 0.9375 - top-5-accuracy: 0.9957 - val_loss: 0.0613 - val_accuracy: 0.9802 - val_top-5-accuracy: 0.9986\n",
      "Epoch 32/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2203 - accuracy: 0.9348 - top-5-accuracy: 0.9950 - val_loss: 0.1043 - val_accuracy: 0.9631 - val_top-5-accuracy: 0.9992\n",
      "Epoch 33/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2139 - accuracy: 0.9377 - top-5-accuracy: 0.9953 - val_loss: 0.0579 - val_accuracy: 0.9802 - val_top-5-accuracy: 0.9989\n",
      "Epoch 34/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2132 - accuracy: 0.9364 - top-5-accuracy: 0.9956 - val_loss: 0.0607 - val_accuracy: 0.9818 - val_top-5-accuracy: 0.9986\n",
      "Epoch 35/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2040 - accuracy: 0.9393 - top-5-accuracy: 0.9953 - val_loss: 0.0599 - val_accuracy: 0.9837 - val_top-5-accuracy: 0.9986\n",
      "Epoch 36/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2125 - accuracy: 0.9357 - top-5-accuracy: 0.9952 - val_loss: 0.0664 - val_accuracy: 0.9824 - val_top-5-accuracy: 0.9983\n",
      "Epoch 37/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2089 - accuracy: 0.9378 - top-5-accuracy: 0.9950 - val_loss: 0.0672 - val_accuracy: 0.9771 - val_top-5-accuracy: 0.9989\n",
      "Epoch 38/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2099 - accuracy: 0.9359 - top-5-accuracy: 0.9957 - val_loss: 0.0767 - val_accuracy: 0.9724 - val_top-5-accuracy: 0.9997\n",
      "Epoch 39/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2054 - accuracy: 0.9386 - top-5-accuracy: 0.9950 - val_loss: 0.0851 - val_accuracy: 0.9730 - val_top-5-accuracy: 0.9992\n",
      "Epoch 40/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2103 - accuracy: 0.9361 - top-5-accuracy: 0.9958 - val_loss: 0.0499 - val_accuracy: 0.9843 - val_top-5-accuracy: 0.9989\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2123 - accuracy: 0.9364 - top-5-accuracy: 0.9957 - val_loss: 0.0706 - val_accuracy: 0.9758 - val_top-5-accuracy: 0.9989\n",
      "Epoch 42/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.1987 - accuracy: 0.9407 - top-5-accuracy: 0.9960 - val_loss: 0.0877 - val_accuracy: 0.9694 - val_top-5-accuracy: 0.9989\n",
      "Epoch 43/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2156 - accuracy: 0.9356 - top-5-accuracy: 0.9953 - val_loss: 0.0579 - val_accuracy: 0.9810 - val_top-5-accuracy: 0.9992\n",
      "Epoch 44/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2056 - accuracy: 0.9381 - top-5-accuracy: 0.9959 - val_loss: 0.0590 - val_accuracy: 0.9780 - val_top-5-accuracy: 0.9994\n",
      "Epoch 45/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.1936 - accuracy: 0.9415 - top-5-accuracy: 0.9963 - val_loss: 0.0568 - val_accuracy: 0.9851 - val_top-5-accuracy: 0.9992\n",
      "Epoch 46/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2029 - accuracy: 0.9386 - top-5-accuracy: 0.9957 - val_loss: 0.0651 - val_accuracy: 0.9793 - val_top-5-accuracy: 0.9986\n",
      "Epoch 47/50\n",
      "2042/2042 [==============================] - 225s 110ms/step - loss: 0.2096 - accuracy: 0.9371 - top-5-accuracy: 0.9951 - val_loss: 0.0645 - val_accuracy: 0.9785 - val_top-5-accuracy: 0.9989\n",
      "Epoch 48/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2007 - accuracy: 0.9391 - top-5-accuracy: 0.9962 - val_loss: 0.0898 - val_accuracy: 0.9716 - val_top-5-accuracy: 0.9983\n",
      "Epoch 49/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2069 - accuracy: 0.9361 - top-5-accuracy: 0.9955 - val_loss: 0.0936 - val_accuracy: 0.9691 - val_top-5-accuracy: 0.9986\n",
      "Epoch 50/50\n",
      "2042/2042 [==============================] - 224s 110ms/step - loss: 0.2108 - accuracy: 0.9357 - top-5-accuracy: 0.9956 - val_loss: 0.0751 - val_accuracy: 0.9760 - val_top-5-accuracy: 0.9983\n",
      "195/195 [==============================] - 7s 34ms/step - loss: 0.0666 - accuracy: 0.9791 - top-5-accuracy: 0.9984\n",
      "Test accuracy: 97.91%\n",
      "Test top 5 accuracy: 99.84%\n"
     ]
    }
   ],
   "source": [
    "# Run experiments with the vanilla ViT\n",
    "vit = create_vit_classifier(vanilla=True)\n",
    "history = run_experiment(vit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2042/2042 [==============================] - 250s 117ms/step - loss: 1.4083 - accuracy: 0.6236 - top-5-accuracy: 0.8829 - val_loss: 0.3523 - val_accuracy: 0.8878 - val_top-5-accuracy: 0.9915\n",
      "Epoch 2/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.5716 - accuracy: 0.8191 - top-5-accuracy: 0.9782 - val_loss: 0.2834 - val_accuracy: 0.9110 - val_top-5-accuracy: 0.9950\n",
      "Epoch 3/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.4254 - accuracy: 0.8666 - top-5-accuracy: 0.9876 - val_loss: 0.1230 - val_accuracy: 0.9587 - val_top-5-accuracy: 0.9986\n",
      "Epoch 4/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.3106 - accuracy: 0.9054 - top-5-accuracy: 0.9926 - val_loss: 0.1232 - val_accuracy: 0.9614 - val_top-5-accuracy: 0.9981\n",
      "Epoch 5/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.2796 - accuracy: 0.9151 - top-5-accuracy: 0.9937 - val_loss: 0.1256 - val_accuracy: 0.9587 - val_top-5-accuracy: 0.9986\n",
      "Epoch 6/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.2451 - accuracy: 0.9248 - top-5-accuracy: 0.9945 - val_loss: 0.1227 - val_accuracy: 0.9609 - val_top-5-accuracy: 0.9972\n",
      "Epoch 7/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.2328 - accuracy: 0.9296 - top-5-accuracy: 0.9949 - val_loss: 0.0865 - val_accuracy: 0.9702 - val_top-5-accuracy: 0.9986\n",
      "Epoch 8/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.2179 - accuracy: 0.9348 - top-5-accuracy: 0.9958 - val_loss: 0.0869 - val_accuracy: 0.9719 - val_top-5-accuracy: 0.9992\n",
      "Epoch 9/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.2083 - accuracy: 0.9381 - top-5-accuracy: 0.9952 - val_loss: 0.0791 - val_accuracy: 0.9724 - val_top-5-accuracy: 0.9992\n",
      "Epoch 10/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.1966 - accuracy: 0.9410 - top-5-accuracy: 0.9964 - val_loss: 0.0753 - val_accuracy: 0.9746 - val_top-5-accuracy: 0.9986\n",
      "Epoch 11/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.2032 - accuracy: 0.9407 - top-5-accuracy: 0.9956 - val_loss: 0.0689 - val_accuracy: 0.9796 - val_top-5-accuracy: 0.9992\n",
      "Epoch 12/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.1977 - accuracy: 0.9414 - top-5-accuracy: 0.9957 - val_loss: 0.0876 - val_accuracy: 0.9758 - val_top-5-accuracy: 0.9986\n",
      "Epoch 13/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1900 - accuracy: 0.9431 - top-5-accuracy: 0.9965 - val_loss: 0.1061 - val_accuracy: 0.9658 - val_top-5-accuracy: 0.9983\n",
      "Epoch 14/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1912 - accuracy: 0.9431 - top-5-accuracy: 0.9962 - val_loss: 0.0507 - val_accuracy: 0.9832 - val_top-5-accuracy: 0.9997\n",
      "Epoch 15/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1805 - accuracy: 0.9471 - top-5-accuracy: 0.9963 - val_loss: 0.0954 - val_accuracy: 0.9647 - val_top-5-accuracy: 0.9992\n",
      "Epoch 16/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1889 - accuracy: 0.9435 - top-5-accuracy: 0.9964 - val_loss: 0.0746 - val_accuracy: 0.9777 - val_top-5-accuracy: 0.9989\n",
      "Epoch 17/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1823 - accuracy: 0.9461 - top-5-accuracy: 0.9968 - val_loss: 0.0887 - val_accuracy: 0.9700 - val_top-5-accuracy: 0.9994\n",
      "Epoch 18/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1862 - accuracy: 0.9453 - top-5-accuracy: 0.9961 - val_loss: 0.0710 - val_accuracy: 0.9785 - val_top-5-accuracy: 0.9983\n",
      "Epoch 19/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1786 - accuracy: 0.9481 - top-5-accuracy: 0.9966 - val_loss: 0.0496 - val_accuracy: 0.9848 - val_top-5-accuracy: 0.9983\n",
      "Epoch 20/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1834 - accuracy: 0.9455 - top-5-accuracy: 0.9964 - val_loss: 0.0637 - val_accuracy: 0.9813 - val_top-5-accuracy: 0.9981\n",
      "Epoch 21/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1771 - accuracy: 0.9460 - top-5-accuracy: 0.9967 - val_loss: 0.0622 - val_accuracy: 0.9807 - val_top-5-accuracy: 0.9986\n",
      "Epoch 22/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1759 - accuracy: 0.9493 - top-5-accuracy: 0.9961 - val_loss: 0.0642 - val_accuracy: 0.9796 - val_top-5-accuracy: 0.9983\n",
      "Epoch 23/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1753 - accuracy: 0.9487 - top-5-accuracy: 0.9969 - val_loss: 0.0828 - val_accuracy: 0.9810 - val_top-5-accuracy: 0.9989\n",
      "Epoch 24/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1717 - accuracy: 0.9489 - top-5-accuracy: 0.9969 - val_loss: 0.0793 - val_accuracy: 0.9774 - val_top-5-accuracy: 0.9986\n",
      "Epoch 25/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1729 - accuracy: 0.9493 - top-5-accuracy: 0.9968 - val_loss: 0.0462 - val_accuracy: 0.9835 - val_top-5-accuracy: 0.9986\n",
      "Epoch 26/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1767 - accuracy: 0.9469 - top-5-accuracy: 0.9969 - val_loss: 0.0737 - val_accuracy: 0.9791 - val_top-5-accuracy: 0.9983\n",
      "Epoch 27/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.1680 - accuracy: 0.9505 - top-5-accuracy: 0.9974 - val_loss: 0.0452 - val_accuracy: 0.9846 - val_top-5-accuracy: 0.9983\n",
      "Epoch 28/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1664 - accuracy: 0.9522 - top-5-accuracy: 0.9968 - val_loss: 0.0607 - val_accuracy: 0.9815 - val_top-5-accuracy: 0.9983\n",
      "Epoch 29/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1726 - accuracy: 0.9497 - top-5-accuracy: 0.9968 - val_loss: 0.0565 - val_accuracy: 0.9810 - val_top-5-accuracy: 0.9989\n",
      "Epoch 30/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1677 - accuracy: 0.9504 - top-5-accuracy: 0.9968 - val_loss: 0.0895 - val_accuracy: 0.9650 - val_top-5-accuracy: 0.9986\n",
      "Epoch 31/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1640 - accuracy: 0.9509 - top-5-accuracy: 0.9969 - val_loss: 0.0607 - val_accuracy: 0.9771 - val_top-5-accuracy: 0.9989\n",
      "Epoch 32/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1635 - accuracy: 0.9512 - top-5-accuracy: 0.9970 - val_loss: 0.0785 - val_accuracy: 0.9738 - val_top-5-accuracy: 0.9986\n",
      "Epoch 33/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1681 - accuracy: 0.9508 - top-5-accuracy: 0.9972 - val_loss: 0.0612 - val_accuracy: 0.9832 - val_top-5-accuracy: 0.9989\n",
      "Epoch 34/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1652 - accuracy: 0.9514 - top-5-accuracy: 0.9970 - val_loss: 0.0624 - val_accuracy: 0.9810 - val_top-5-accuracy: 0.9986\n",
      "Epoch 35/50\n",
      "2042/2042 [==============================] - 239s 117ms/step - loss: 0.1704 - accuracy: 0.9497 - top-5-accuracy: 0.9970 - val_loss: 0.0517 - val_accuracy: 0.9802 - val_top-5-accuracy: 0.9989\n",
      "Epoch 36/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1665 - accuracy: 0.9500 - top-5-accuracy: 0.9968 - val_loss: 0.0634 - val_accuracy: 0.9799 - val_top-5-accuracy: 0.9989\n",
      "Epoch 37/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1618 - accuracy: 0.9508 - top-5-accuracy: 0.9972 - val_loss: 0.0824 - val_accuracy: 0.9730 - val_top-5-accuracy: 0.9978\n",
      "Epoch 38/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.1610 - accuracy: 0.9521 - top-5-accuracy: 0.9967 - val_loss: 0.0965 - val_accuracy: 0.9683 - val_top-5-accuracy: 0.9983\n",
      "Epoch 39/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1639 - accuracy: 0.9521 - top-5-accuracy: 0.9971 - val_loss: 0.0661 - val_accuracy: 0.9799 - val_top-5-accuracy: 0.9989\n",
      "Epoch 40/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1632 - accuracy: 0.9517 - top-5-accuracy: 0.9973 - val_loss: 0.0554 - val_accuracy: 0.9804 - val_top-5-accuracy: 0.9989\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.1545 - accuracy: 0.9550 - top-5-accuracy: 0.9969 - val_loss: 0.0735 - val_accuracy: 0.9799 - val_top-5-accuracy: 0.9989\n",
      "Epoch 42/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1677 - accuracy: 0.9515 - top-5-accuracy: 0.9968 - val_loss: 0.0452 - val_accuracy: 0.9851 - val_top-5-accuracy: 0.9994\n",
      "Epoch 43/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.1595 - accuracy: 0.9539 - top-5-accuracy: 0.9965 - val_loss: 0.0787 - val_accuracy: 0.9741 - val_top-5-accuracy: 0.9983\n",
      "Epoch 44/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1628 - accuracy: 0.9520 - top-5-accuracy: 0.9971 - val_loss: 0.0760 - val_accuracy: 0.9782 - val_top-5-accuracy: 0.9986\n",
      "Epoch 45/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1656 - accuracy: 0.9511 - top-5-accuracy: 0.9974 - val_loss: 0.0752 - val_accuracy: 0.9722 - val_top-5-accuracy: 0.9983\n",
      "Epoch 46/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1650 - accuracy: 0.9507 - top-5-accuracy: 0.9972 - val_loss: 0.0568 - val_accuracy: 0.9810 - val_top-5-accuracy: 0.9983\n",
      "Epoch 47/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1583 - accuracy: 0.9533 - top-5-accuracy: 0.9973 - val_loss: 0.0538 - val_accuracy: 0.9829 - val_top-5-accuracy: 0.9983\n",
      "Epoch 48/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1581 - accuracy: 0.9527 - top-5-accuracy: 0.9968 - val_loss: 0.0518 - val_accuracy: 0.9804 - val_top-5-accuracy: 0.9997\n",
      "Epoch 49/50\n",
      "2042/2042 [==============================] - 238s 116ms/step - loss: 0.1614 - accuracy: 0.9513 - top-5-accuracy: 0.9970 - val_loss: 0.0573 - val_accuracy: 0.9782 - val_top-5-accuracy: 0.9992\n",
      "Epoch 50/50\n",
      "2042/2042 [==============================] - 238s 117ms/step - loss: 0.1579 - accuracy: 0.9532 - top-5-accuracy: 0.9974 - val_loss: 0.0397 - val_accuracy: 0.9873 - val_top-5-accuracy: 0.9989\n",
      "195/195 [==============================] - 7s 37ms/step - loss: 0.0335 - accuracy: 0.9878 - top-5-accuracy: 0.9997\n",
      "Test accuracy: 98.78%\n",
      "Test top 5 accuracy: 99.97%\n"
     ]
    }
   ],
   "source": [
    "# Run experiments with the Shifted Patch Tokenization and Locality Self Attention modified ViT\n",
    "vit_sl = create_vit_classifier(vanilla=False)\n",
    "history = run_experiment(vit_sl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9J2omGo34Hz"
   },
   "source": [
    "*TODO: omówienie (100 słów)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "omówienie will be here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
